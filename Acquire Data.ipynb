{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b9acdc3-3b4d-451e-bcb7-cc489fe5e1d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Phase 1: Acquire Data\n",
    "\n",
    "This notebook will get data about your Azure Databricks VM usage.  It looks at the last 12 days of the\n",
    "[Azure Activity Logs](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/activity-log) for the\n",
    "Databricks managed resource groups to determine what VMs are created and how long they are used.\n",
    "\n",
    "Through this process, we will build a tiny little lakehouse to store the Databricks VM usage data in its\n",
    "various stages of processing.  The Azure Activity Log data is saved in its raw form as JSON data.\n",
    "This notebook then uses Spark to transform that raw data into Delta files in a Bronze, Silver, and Gold\n",
    "layer.  Each of these steps further transforms the data and prepares it for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "769adbda-18ac-48fc-8c84-0fd1650bb17d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The Azure Activity Log can be accessed through the Azure Management API.  Rather than making raw REST calls, we'll use the \n",
    "[Azure Libraries for Python](https://learn.microsoft.com/en-us/azure/developer/python/sdk/azure-sdk-overview).  We can use the `%pip`\n",
    "magic command to install them in our current Python session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eef9083c-db94-415b-9cb6-669cdb11e35d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-identity azure-mgmt-monitor azure-mgmt-resourcegraph azure-mgmt-resource azure-mgmt-compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e300508e-6c41-4a4f-8958-4099073735cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configuration\n",
    "\n",
    "These are the configuration settings you will need to adjust to specify how the data acquisition process should operate.\n",
    "\n",
    "**`service_principal`** contains the information about the service principal that you will use to connect to the Azure Management API.\n",
    "If you used the `az ad sp create-for-rbac` CLI command to create the service principal, then you can just straight copy and paste the\n",
    "output of that command here.\n",
    "\n",
    "**`subscription_ids`** is a Python list of the subscription ID's for which you want to gather Databricks VM usage data.  If you leave\n",
    "the list empty, then this process will pull Databricks VM usage data for all subscriptions to which the service principal has access.\n",
    "\n",
    "**`delta_location`** is the path where the Databricks VM usage data will be saved.  It can be a location on the DBFS or it can use\n",
    "any standard HDFS path (*e.g.* an `abfss://` path)\n",
    "\n",
    "**`days_to_pull`** specifies how many days into the past the Azure Activity Log should be queried.  By default, the Azure Activity Log\n",
    "retains 90 days of data.  Specifying a lower number will speed up the process because there will be fewer queries made.  However,\n",
    "you will want to pull several days of data to get a clearer picture of the usage patterns.\n",
    "\n",
    "**`query_window_hours`** controls how many hours will be covered by each query to the Azure Activity Log.  Instead of issuing one, big\n",
    "query to the Activity Log to pull all of the history at once, this notebook will break it up into multiple queries.  If you don't have\n",
    "a lot of Databricks VM activity, you can set this number higher.  For example, setting it to \"24\" will issue one query per day.  If\n",
    "you have a high volume of Databricks VM activity, set this number lower.  This will break the querying process into smaller, more\n",
    "reliable chunks.  If you're not sure, just leave it at 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "624b006b-5ce9-4bd8-a8f4-19894bb3545c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "service_principal = {\n",
    "  \"appId\": \"<your App ID / Client ID (should be a guid)>\",\n",
    "  \"displayName\": \"<display name for your SP (optional)>\",\n",
    "  \"password\": \"<Password / Secret (probably a 34-byte string)>\",\n",
    "  \"tenant\": \"<your Tenant ID (should be a guid)>\"\n",
    "}\n",
    "\n",
    "subscription_ids = []\n",
    "\n",
    "delta_location = \"dbfs:/vm-observation\"\n",
    "\n",
    "days_to_pull = 12\n",
    "query_window_hours = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17ca2c8d-0dca-435f-b60d-fa7f3fea41e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Raw Data\n",
    "\n",
    "Read the Azure Activity Logs and save the results as JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "856f7dde-68a9-42d9-b7c9-e43e04fcf82b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import azure.mgmt.resourcegraph as arg\n",
    "from datetime import datetime, timedelta\n",
    "from azure.identity import ClientSecretCredential, AzureCliCredential\n",
    "from azure.mgmt.resource import SubscriptionClient\n",
    "from azure.mgmt.monitor import MonitorManagementClient\n",
    "\n",
    "\n",
    "credential = ClientSecretCredential(service_principal[\"tenant\"], service_principal[\"appId\"], service_principal[\"password\"])\n",
    "#credential = AzureCliCredential() # (Optional... if you want to use Azure CLI to login instead of using a service principal)\n",
    "\n",
    "while delta_location.endswith(\"/\"):\n",
    "    delta_location = delta_location[:-1]\n",
    "\n",
    "# If we were not given a list of subscriptions, just get all of the ones the SP has access to\n",
    "if not subscription_ids:\n",
    "    subscriptions_client = SubscriptionClient(credential)\n",
    "    subscription_ids = [s.as_dict()['subscription_id'] for s in subscriptions_client.subscriptions.list()]\n",
    "\n",
    "\n",
    "def get_adbx_workspaces():\n",
    "    arg_client = arg.ResourceGraphClient(credential)\n",
    "    query_options = arg.models.QueryRequestOptions(result_format = \"objectArray\")\n",
    "\n",
    "    query_string = 'resources | where type == \"microsoft.databricks/workspaces\" | project workspaceId = id, name, subscriptionId, sku = sku.name, managedResourceGroupId = properties.managedResourceGroupId, location'\n",
    "    query_request = arg.models.QueryRequest(subscriptions=subscription_ids, query=query_string, options=query_options)\n",
    "    results = arg_client.resources(query_request)\n",
    "\n",
    "    output = results.as_dict()[\"data\"]\n",
    "    for o in output:\n",
    "        o[\"managedResourceGroupName\"] = o[\"managedResourceGroupId\"].split('/')[-1]\n",
    "        \n",
    "        monitor_client = MonitorManagementClient(credential, o[\"subscriptionId\"])\n",
    "        diagnostic_settings = monitor_client.diagnostic_settings.list(o[\"workspaceId\"])\n",
    "        o[\"diagnostic_settings\"] = [s.as_dict() for s in diagnostic_settings]\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def get_activity_log_for_workspace(workspace, date):\n",
    "    print(f\"{datetime.now()}\\n - Getting activity log for workspace \\\"{workspace['name']}\\\"    {date.date()}  {date.hour}:00\")\n",
    "    monitor_client = MonitorManagementClient(credential, workspace[\"subscriptionId\"])\n",
    "    \n",
    "    start_time = date - timedelta(hours=query_window_hours) + timedelta(microseconds=1)\n",
    "    end_time = date\n",
    "    filter = f\"eventTimestamp ge '{start_time.isoformat()}' and eventTimestamp le '{end_time.isoformat()}' and resourceGroupName eq '{workspace['managedResourceGroupName']}' and resourceType eq 'Microsoft.Compute/virtualMachines'\"\n",
    "    \n",
    "    log_entries = monitor_client.activity_logs.list(filter)\n",
    "    return [i.as_dict() for i in log_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b79df55-01c8-4b43-a24e-9f7fe52db86b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "workspaces = get_adbx_workspaces()\n",
    "dbutils.fs.put(f\"{delta_location}/raw/workspaces/workspaces.json\", json.dumps({\"workspaces\": workspaces, \"capture_time\": datetime.now().isoformat() + \"Z\"}))\n",
    "\n",
    "ws_dict = {ws['managedResourceGroupName']: ws for ws in workspaces}\n",
    "timestamp = datetime.now()\n",
    "\n",
    "i = 0\n",
    "for rg_name in ws_dict:\n",
    "    ws = ws_dict[rg_name]\n",
    "    i = i + 1\n",
    "\n",
    "    print(f\"\\n*************************\\nProcessing workspace {ws['name']}   ({i} of {len(ws_dict)})\")\n",
    "    json_path = f\"{delta_location}/raw/activity_logs/subscription={ws['subscriptionId']}/rg={rg_name}\"\n",
    "\n",
    "    for h in range(0, 24 * days_to_pull, query_window_hours):\n",
    "        date = timestamp - timedelta(hours=h)\n",
    "        logs = get_activity_log_for_workspace(ws, date)\n",
    "\n",
    "        print(f\" - Parsing response body JSON\")\n",
    "        for log in logs:\n",
    "            if \"properties\" in log:\n",
    "                if \"responseBody\" in log[\"properties\"]:\n",
    "                    response_body = json.loads(log[\"properties\"][\"responseBody\"])\n",
    "                    log[\"properties\"][\"responseBody\"] = response_body\n",
    "\n",
    "        json_filename = f\"{json_path}/{date.date()}-{date.hour}.json\"\n",
    "        print(f\" - Saving log to file: {json_filename}\")\n",
    "        dbutils.fs.put(json_filename, json.dumps({\"workspace\": ws, \"logs\": logs, \"capture_time\": datetime.now().isoformat() + \"Z\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cb028ba-a202-4872-b9ee-b7fee5b8d27e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Get VM SKU Information\n",
    "In addition to the VM usage logs, we also need to get some information about the VM SKU's.  We will pull it from the Azure API just to be sure\n",
    "we always have the latest data.  The available VM types can vary by subscription and by region so we will have to query with multiple sets of\n",
    "parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e791230-bc61-4494-b264-e69b77368aab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.mgmt.compute import ComputeManagementClient\n",
    "\n",
    "sub_locs = [(ws['subscriptionId'], ws['location']) for ws in workspaces]\n",
    "sub_locs = list(set(sub_locs))\n",
    "\n",
    "for sub_loc in sub_locs:\n",
    "    print(f\"Getting VM SKUs for subscription {sub_loc[0]} in {sub_loc[1]} region\")\n",
    "    compute_client = ComputeManagementClient(credential, sub_loc[0])\n",
    "    results = compute_client.virtual_machine_sizes.list(sub_loc[1])\n",
    "    results = [i.as_dict() for i in results]\n",
    "\n",
    "    for r in results:\n",
    "        r[\"subscription_id\"] = sub_loc[0]\n",
    "        r[\"location\"] = sub_loc[1]\n",
    "\n",
    "    json_path = f\"{delta_location}/raw/vm_skus/subscription={sub_loc[0]}\"\n",
    "    json_filename = f\"{json_path}/{sub_loc[1]}.json\"\n",
    "    dbutils.fs.put(json_filename, json.dumps(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b934d773-bd7a-4019-a0dd-c2ba80ddb7cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Bronze Layer\n",
    "\n",
    "This is the first step in transforming our raw data.  We will simply read the JSON files and apply only the bare minimum of transformations\n",
    "before converting it to Delta files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1dd4de7-39af-4641-ac8c-a0c64e4f723b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "\n",
    "df = spark.read \\\n",
    "        .format(\"json\") \\\n",
    "        .load(delta_location + \"/raw/activity_logs\") \\\n",
    "        .selectExpr(\"workspace.workspaceId\", \"workspace.name AS workspaceName\", \"workspace.sku AS workspaceSku\", \"workspace.subscriptionId\",\n",
    "                    \"workspace.managedResourceGroupId\", \"CAST(capture_time AS TIMESTAMP) AS capture_time\", \"explode(logs) AS log_entry\") \\\n",
    "        .selectExpr(\"CAST(log_entry.event_timestamp AS TIMESTAMP) AS event_timestamp\", \"lower(split(log_entry.resource_id, '/')[8]) as vm_id\", \"*\") \\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bee3f741-30a3-4baf-8878-55d22999fa3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are some columns that we definitely won't need for analysis and they have some challenging column names.  So we will make life\n",
    "easier for us and just drop those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d45cab6-54ce-462a-9775-3ef56fa93496",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "claim_cols = df.select(\"log_entry.claims.*\").columns\n",
    "\n",
    "for col in claim_cols:\n",
    "    if col.startswith(\"http\") and \":/\" in col:\n",
    "        df = df.withColumn(\"log_entry\", F.col(\"log_entry\").dropFields(\"claims.`\" + col + \"`\"))\n",
    "\n",
    "df = df.withColumn(\"log_entry\", F.col(\"log_entry\").dropFields(\"properties.responseBody.identity\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68fdb13a-035c-41b2-86e9-a9ea46c759b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Each of the Azure tags applied to a Databricks VM is represented as its own field in a nested column.  However, Delta does not allow the names of two\n",
    "columns to vary only by case.  For example, Delta won't let you have a column called `project_name` and `Project_Name` in the same table.  Across a large\n",
    "organization, it's easy for Azure tags to have inconsistent casing in their names.  Therefore, as we convert the raw data to Delta, we have to find\n",
    "Azure tag columns that have the same name but different letter casing and combine those into one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "295aa372-70d9-4411-9d3f-9cb65f053256",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tagnames = df.select(\"log_entry.properties.responseBody.tags.*\").columns\n",
    "\n",
    "instance_pool_tagname = \"DatabricksInstancePoolId\"\n",
    "if instance_pool_tagname not in tagnames:\n",
    "    print(\"Instance Pool ID column not in JSON schema.  Adding it to Delta file.\")\n",
    "    df = df.withColumn(\"log_entry\", F.col(\"log_entry\").withField(\"properties.responseBody.tags.\" + instance_pool_tagname, F.lit(None).astype(\"string\"))) \n",
    "\n",
    "\n",
    "tagname_groups = {}\n",
    "for tagname in tagnames:\n",
    "    tagname_groups.setdefault(tagname.lower(), []).append(tagname)\n",
    "\n",
    "for key in tagname_groups:\n",
    "    values = tagname_groups[key]\n",
    "    if len(values) > 1:\n",
    "        keep = values[0]\n",
    "        for i in range(1, len(values)):\n",
    "            print(f\"combine tag \\\"{values[i]}\\\" into tag \\\"{keep}\\\"\")\n",
    "            df = df.withColumn(\"log_entry\",\n",
    "                F.col(\"log_entry\").withField(\"properties.responseBody.tags.\" + keep,\n",
    "                                                F.coalesce(\n",
    "                                                    F.col(\"log_entry.properties.responseBody.tags.\" + keep),\n",
    "                                                    F.col(\"log_entry.properties.responseBody.tags.\" + values[i])\n",
    "                                                )\n",
    "                                            )\n",
    "            ) \n",
    "            df = df.withColumn(\"log_entry\", F.col(\"log_entry\").dropFields(\"properties.responseBody.tags.\" + values[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcb87487-b0a0-47b9-8702-c15c118abf94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Delta has some restrictions on what characters can be used in column names.  Most notabaly, this includes spaces.  We will look through\n",
    "all of the Azure tag columns and change any invalid characters to underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e73988dc-a5e9-476b-9227-d389889638fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tagnames = df.select(\"log_entry.properties.responseBody.tags.*\").columns\n",
    "bad_chars = \" ,;{}()\\n\\t=\"\n",
    "\n",
    "for tagname in tagnames:\n",
    "    for c in bad_chars:\n",
    "        if c in tagname:\n",
    "            new_tagname = tagname.replace(c, \"_\")\n",
    "            print(f\"Change tag \\\"{tagname}\\\" to \\\"{new_tagname}\")\n",
    "\n",
    "            if new_tagname in tagnames:\n",
    "                df = df.withColumn(\"log_entry\",\n",
    "                    F.col(\"log_entry\").withField(\"properties.responseBody.tags.\" + new_tagname,\n",
    "                                                    F.coalesce(\n",
    "                                                        F.col(\"log_entry.properties.responseBody.tags.\" + new_tagname),\n",
    "                                                        F.col(\"log_entry.properties.responseBody.tags.\" + tagname)\n",
    "                                                    )\n",
    "                                                )\n",
    "                ) \n",
    "            else:\n",
    "                df = df.withColumn(\"log_entry\",\n",
    "                    F.col(\"log_entry\").withField(\n",
    "                        \"properties.responseBody.tags.\" + new_tagname,\n",
    "                        F.col(\"log_entry.properties.responseBody.tags.\" + tagname)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            df = df.withColumn(\"log_entry\", F.col(\"log_entry\").dropFields(\"properties.responseBody.tags.`\" + tagname + \"`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36e4b4c5-a5ab-40d6-84ed-3ccff01ad5b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_location + \"/bronze/activity_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c00c743-b7f3-485e-bc94-4c9afb2d7f7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### VM SKU Information\n",
    "We also need to convert our VM SKU data from JSON to Delta!  This is much simpler, though, because the data is not complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26332e3e-8e6b-48c5-99cc-3d2c20d54e44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "        .format(\"json\") \\\n",
    "        .load(delta_location + \"/raw/vm_skus\")\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_location + \"/bronze/vm_skus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02f93ba2-5a01-4892-b2e6-d279a7b6696c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Silver Layer\n",
    "\n",
    "Now that we have our data in the easy-to-use Delta format, we can begin transforming it into a format that is more useful for analysis.\n",
    "The raw data contains a log of all of the VM operations that Azure performed in the Databricks-managed resource groups.  However many\n",
    "of those operations aren't useful for our analysis.  We only want to know when a new VM was created or an existing VM was deleted.\n",
    "(Though we won't use it for this analysis, we'll also record when spot instances are evicted because that could be interesting to\n",
    "study.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97433ce1-2a36-42e4-99ed-c0981c277bcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = spark.read.format(\"delta\").load(delta_location + \"/bronze/activity_logs\")\n",
    "\n",
    "# Get rows for the creation, eviction, or deletion of a VM\n",
    "activities = df \\\n",
    "                .filter(\"\"\"\n",
    "                    (log_entry.properties.responseBody IS NOT NULL AND log_entry.resource_type.value != 'Microsoft.Compute/virtualMachines/extensions')\n",
    "                    OR lower(log_entry.operation_name.value) LIKE '%spot%'\n",
    "                    OR (log_entry.operation_name.value == 'Microsoft.Compute/virtualMachines/delete' AND log_entry.event_name.value == 'EndRequest')\n",
    "                \"\"\") \\\n",
    "                .selectExpr(\"vm_id\", \"event_timestamp\", \"log_entry.properties.statusCode\", \"log_entry.operation_name.value AS operation\", \"subscriptionId\",\n",
    "                            \"workspaceName\", \"log_entry.properties.responseBody.properties.hardwareProfile.vmSize\", \"log_entry.properties.responseBody.properties.priority\",\n",
    "                            \"log_entry.properties.responseBody.location\", \"log_entry.properties.responseBody.tags\") \\\n",
    "                .orderBy(\"vm_id\", \"event_timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e6b9361-17d2-4e99-84c8-777782a7fdbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For our analysis, we need the Azure tags associated with the VM.  This tells us which job, cluster, or pool the VM was part of.  However, we only\n",
    "get the Azure tag data when a VM is created. If a VM was started before the period of time we are studying, we won't have that tag data in our logs.\n",
    "Therefore, we will just throw out any data for VM's that are missing this critical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a1a9734-48bf-467e-9a4a-00fc6f547235",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove all information about VMs if we don't have any tags for it\n",
    "activities.createOrReplaceTempView(\"activities_table\")\n",
    "\n",
    "no_starts = spark.sql(\"SELECT vm_id, COUNT(*), MIN(event_timestamp), MAX(event_timestamp) FROM activities_table WHERE vm_id NOT IN (SELECT vm_id FROM activities_table WHERE tags IS NOT NULL) GROUP BY vm_id ORDER BY 3 DESC\")\n",
    "print(f\"\\nRemoving data for {no_starts.count():,} VMs because there is no creation data in the logs.\\n\")\n",
    "no_starts.show(truncate=False)\n",
    "\n",
    "activities = spark.sql(\"SELECT * FROM activities_table WHERE vm_id IN (SELECT vm_id FROM activities_table WHERE tags IS NOT NULL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fffcbf3d-49f4-4702-bc98-431ff35013dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Since not all data points (like Azure tag data) is present in every log entry, we will order the logs by VM ID and timestamp.  We will carry forward\n",
    "these data points to all future rows that are missing the data.\n",
    "\n",
    "That is to say, if we have sparse data like this...\n",
    "\n",
    "| VM ID | Timestamp | VM Size  |\n",
    "|-------|-----------|----------|\n",
    "| 1234  | 13:38:34  | E8ds_v4  |\n",
    "| 1234  | 13:42:18  | *null*   |\n",
    "| 1234  | 13:46:12  | *null*   |\n",
    "| 1234  | 13:47:09  | *null*   |\n",
    "| 1234  | 13:49:58  | E16ds_v4 |\n",
    "| 1234  | 13:50:02  | *null*   |\n",
    "| 5678  | 12:17:24  | DS3_v3   |\n",
    "| 5678  | 12:22:32  | *null*   |\n",
    "| 5678  | 12:34:02  | *null*   |\n",
    "\n",
    "... we will carry forward the values in the sparse column so it looks like this:\n",
    "\n",
    "| VM ID | Timestamp | VM Size  |\n",
    "|-------|-----------|----------|\n",
    "| 1234  | 13:38:34  | E8ds_v4  |\n",
    "| 1234  | 13:42:18  | E8ds_v4  |\n",
    "| 1234  | 13:46:12  | E8ds_v4  |\n",
    "| 1234  | 13:47:09  | E8ds_v4  |\n",
    "| 1234  | 13:49:58  | E16ds_v4 |\n",
    "| 1234  | 13:50:02  | E16ds_v4 |\n",
    "| 5678  | 12:17:24  | DS3_v3   |\n",
    "| 5678  | 12:22:32  | DS3_v3   |\n",
    "| 5678  | 12:34:02  | DS3_v3   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fced932c-099d-4989-b5c4-4ceb6d9fff9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"vm_id\").orderBy(\"event_timestamp\")\n",
    "\n",
    "activities = activities \\\n",
    "                .withColumn(\"activity_number\", F.row_number().over(window_spec)) \\\n",
    "                .withColumn(\"event_end\", F.lead(\"event_timestamp\").over(window_spec) - F.expr('INTERVAL 1 MICROSECOND')) \\\n",
    "                .withColumn(\"vmSize\", F.last(\"vmSize\", True).over(window_spec)) \\\n",
    "                .withColumn(\"priority\", F.last(\"priority\", True).over(window_spec)) \\\n",
    "                .withColumn(\"location\", F.last(\"location\", True).over(window_spec)) \\\n",
    "                .withColumn(\"tags\", F.last(\"tags\", True).over(window_spec))\n",
    "\n",
    "print(f\"Rows: {activities.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b33a312-c83f-4375-8878-39d392d385be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Silver processing complete!  Write out this version of the data to its own Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9416118-3574-47d6-b45e-4de19d7cbe22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "activities.write.format(\"delta\").mode(\"overwrite\").save(delta_location + \"/silver/activities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "500f5183-5ef5-4833-a31d-68c8623caf32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### VM SKU Information\n",
    "This will be the final step in processing the VM SKU data.  We will interpret the SKU name to get additional information about the VM series.\n",
    "\n",
    "For information on how the VM SKU's are named, see this document: [Azure virtual machine sizes naming conventions](https://learn.microsoft.com/en-us/azure/virtual-machines/vm-naming-conventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d146ebc9-812a-4ec6-88cb-dab5b75fd7b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(delta_location + \"/bronze/vm_skus\")\n",
    "\n",
    "df = df \\\n",
    "        .drop(\"subscription\") \\\n",
    "        .withColumn(\"memory_in_gb\", F.expr(\"memory_in_mb / 1024\")) \\\n",
    "        .withColumn(\"name_without_version\", F.expr(\"REGEXP_REPLACE(name, '\\_v[0-9]+', '')\")) \\\n",
    "        .withColumn(\"name_parts\", F.split(\"name\", \"_\")) \\\n",
    "        .withColumn(\"is_promo\", F.expr(\"name LIKE '%_Promo'\")) \\\n",
    "        .withColumn(\"family_group\", F.expr(\"name_parts[0]\")) \\\n",
    "        .withColumn(\"family\", F.expr(\"REGEXP_EXTRACT(name_parts[1], '^[A-z]+', 0)\")) \\\n",
    "        .withColumn(\"name_number\", F.expr(\"REGEXP_EXTRACT(name_parts[1], '^([A-z]+)([0-9]+)', 2)\")) \\\n",
    "        .withColumn(\"constrained_vcpu\", F.expr(\"REGEXP_EXTRACT(name_parts[1], '^([A-z]+)([0-9]+)-([0-9]+)', 3)\")) \\\n",
    "        .withColumn(\"additive_features\", F.expr(\"REGEXP_EXTRACT(name_parts[1], '^([A-z]+)([0-9]+)((-[0-9]+)?)(.*)', 5)\")) \\\n",
    "        .withColumn(\"accelerator_type\", F.expr(\"CASE WHEN name_parts[2] != 'Promo' AND name_parts[2] NOT RLIKE 'v[0-9]+' THEN name_parts[2] ELSE NULL END\")) \\\n",
    "        .withColumn(\"version_number\", F.expr(\"CASE WHEN name_parts[2] RLIKE 'v[0-9]+' THEN name_parts[2] ELSE CASE WHEN name_parts[3] RLIKE 'v[0-9]+' THEN name_parts[3] ELSE 'v1' END END\")) \\\n",
    "        .drop(\"name_parts\")\n",
    "\n",
    "df = df \\\n",
    "        .withColumn(\"is_amd\", F.expr(\"additive_features RLIKE 'a'\")) \\\n",
    "        .withColumn(\"is_block_storage_performance\", F.expr(\"additive_features RLIKE 'b'\")) \\\n",
    "        .withColumn(\"has_local_temp_disk\", F.expr(\"additive_features RLIKE 'd'\")) \\\n",
    "        .withColumn(\"is_isolated\", F.expr(\"additive_features RLIKE 'i'\")) \\\n",
    "        .withColumn(\"is_low_memory\", F.expr(\"additive_features RLIKE 'l'\")) \\\n",
    "        .withColumn(\"is_memory_intensive\", F.expr(\"additive_features RLIKE 'm'\")) \\\n",
    "        .withColumn(\"is_arm\", F.expr(\"additive_features RLIKE 'p'\")) \\\n",
    "        .withColumn(\"is_tiny_memory\", F.expr(\"additive_features RLIKE 't'\")) \\\n",
    "        .withColumn(\"has_premium_storage\", F.expr(\"additive_features RLIKE 's'\")) \\\n",
    "        .withColumn(\"is_confidential\", F.expr(\"additive_features RLIKE 'C'\")) \\\n",
    "        .withColumn(\"is_node_packing\", F.expr(\"additive_features RLIKE 'NP'\"))\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38c5c4ae-f076-4076-adc8-edc91d4e11c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The Azure API does not provide information about which series a VM type belongs to.  So we will copy some data from the docs and use that instead.\n",
    "This may have to be manually updated from time to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fece909e-4070-4330-91bc-6adc20aa5824",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copied from:  https://learn.microsoft.com/en-us/azure/virtual-machines/sizes\n",
    "\n",
    "series_data = [\n",
    "    [\"General purpose\",\n",
    "     \"B, Dsv3, Dv3, Dasv4, Dav4, DSv2, Dv2, Av2, DC, DCv2, Dpdsv5, Dpldsv5, Dpsv5, Dplsv5, Dv4, Dsv4, Ddv4, Ddsv4, Dv5, Dsv5, Ddv5, Ddsv5, Dasv5, Dadsv5\",\n",
    "     \"Balanced CPU-to-memory ratio. Ideal for testing and development, small to medium databases, and low to medium traffic web servers.\"\n",
    "    ],\n",
    "    [\"Compute optimized\",\n",
    "     \"F, Fs, Fsv2, FX\",\n",
    "     \"High CPU-to-memory ratio. Good for medium traffic web servers, network appliances, batch processes, and application servers.\"\n",
    "    ],\n",
    "    [\"Memory optimized\",\n",
    "     \"Esv3, Ev3, Easv4, Eav4, Epdsv5, Epsv5, Ev4, Esv4, Edv4, Edsv4, Ev5, Esv5, Edv5, Edsv5, Easv5, Eadsv5, Mv2, M, DSv2, Dv2\",\n",
    "      \"High memory-to-CPU ratio. Great for relational database servers, medium to large caches, and in-memory analytics.\"\n",
    "    ],\n",
    "    [\"Storage optimized\",\n",
    "     \"Lsv2, Lsv3, Lasv3\",\n",
    "      \"High disk throughput and IO ideal for Big Data, SQL, NoSQL databases, data warehousing and large transactional databases.\"\n",
    "    ],\n",
    "    [\"GPU\",\n",
    "     \"NC, NCv2, NCv3, NCasT4_v3, ND, NDv2, NV, NVv3, NVv4, NDasrA100_v4, NDm_A100_v4\",\n",
    "     \"Specialized virtual machines targeted for heavy graphic rendering and video editing, as well as model training and inferencing (ND) with deep learning. Available with single or multiple GPUs.\"\n",
    "    ],\n",
    "    [\"High performance compute\",\n",
    "     \"HB, HBv2, HBv3, HBv4, HC, HX\",\n",
    "     \"Our fastest and most powerful CPU virtual machines with optional high-throughput network interfaces (RDMA).\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "series_data = [{'series': i[0], 'type': i[1].split(','), 'description': i[2]} for i in series_data]\n",
    "series_data = spark.createDataFrame(series_data)\n",
    "\n",
    "series_data = series_data \\\n",
    "                .withColumn(\"type\", F.explode(\"type\")) \\\n",
    "                .withColumn(\"type\", F.expr(\"TRIM(type)\")) \\\n",
    "                .withColumn(\"family\", F.expr(\"REGEXP_EXTRACT(type, '^[A-Z]+', 0)\")) \\\n",
    "                .withColumn(\"version_number\", F.expr(\"REGEXP_EXTRACT(type, 'v[0-9]+$', 0)\")) \\\n",
    "                .withColumn(\"version_number\", F.expr(\"CASE WHEN version_number == '' THEN 'v1' ELSE version_number END\"))\n",
    "\n",
    "series_data = series_data \\\n",
    "                .groupBy(\"family\", \"version_number\") \\\n",
    "                .agg(F.min(\"series\").alias(\"series\")) \\\n",
    "                .join(series_data.groupBy(\"series\").agg(F.max(\"description\").alias(\"description\")), on=\"series\", how=\"inner\")\n",
    "\n",
    "df = df.join(series_data, on=[\"family\", \"version_number\"], how=\"left_outer\")\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_location + \"/silver/vm_skus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc0e9538-fdc0-458c-a31e-d8a37ba2e0c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Gold Layer\n",
    "\n",
    "For this last step in the data preparation, we want to explode the dataset rows so that there is an individual record for every minute that a VM was running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6979424d-0df5-4142-afd9-0b1720a4519a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "activities = spark.read.format(\"delta\").load(delta_location + \"/silver/activities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb54cfd2-9229-4259-ad64-d6f128de43cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "dates = activities \\\n",
    "            .agg(F.min(\"event_timestamp\").alias(\"min\"), F.max(\"event_timestamp\").alias(\"max\")) \\\n",
    "            .selectExpr(\"*\", \"(TO_UNIX_TIMESTAMP(max) - TO_UNIX_TIMESTAMP(min)) / 60 AS minutes\") \\\n",
    "            .collect()\n",
    "\n",
    "start_date = dates[0][0]\n",
    "start_date = datetime(start_date.year, start_date.month, start_date.day, start_date.hour, start_date.minute)\n",
    "minutes = dates[0][2]\n",
    "\n",
    "observations = spark.range(minutes) \\\n",
    "                    .withColumn(\"date\", F.lit(start_date) + F.expr(\"MAKE_INTERVAL(0, 0, 0, 0, 0, id)\"))\n",
    "\n",
    "\n",
    "observations.alias(\"o\") \\\n",
    "    .join(activities.filter(\"operation NOT LIKE '%delete'\").alias(\"a\"),\n",
    "          observations[\"date\"].between(activities[\"event_timestamp\"], activities[\"event_end\"])) \\\n",
    "    .withColumn(\"JobName\", F.expr(\"REGEXP_REPLACE(REGEXP_REPLACE(a.tags.RunName, '^ADF\\_', ''), '\\_[a-f0-9\\-]{36}$', '')\")) \\\n",
    "    .groupBy(\"o.date\", \"a.subscriptionId\", \"a.workspaceName\", \"a.vmSize\", \"a.priority\", \"a.location\", \"a.tags.ClusterName\", \"a.tags.JobId\",\n",
    "             \"JobName\", \"a.tags.RunName\", \"a.tags.DatabricksInstancePoolId\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"o.date\") \\\n",
    "    .write \\\n",
    "    .format(\"delta\") \\\n",
    "    .save(delta_location + \"/gold/observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab621102-e777-4756-8d2c-ea63f8dda6a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Checks\n",
    "\n",
    "Now that we've built our datasets, let's check them to make sure they conform to our expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e3fdd27-c382-45fa-9522-adb913906ed5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(delta_location + \"/bronze/activity_logs\")\n",
    "activities = spark.read.format(\"delta\").load(delta_location + \"/silver/activities\")\n",
    "observations = spark.read.format(\"delta\").load(delta_location + \"/gold/observations\")\n",
    "\n",
    "print(f\"Bronze Rows: {df.count():,}\")\n",
    "print(f\"Silver Rows: {activities.count():,}\")\n",
    "print(f\"Gold Rows:   {observations.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6fa9cd8-b36e-4e31-8696-b37cf88acafc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df.filter(\"event_timestamp IS NULL\").count())\n",
    "print(df.filter(\"vm_id IS NULL\").count())\n",
    "print(df.filter(\"lower(vm_id) NOT rlike '^[0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f]$'\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13844e17-9dc8-4283-af19-dc74488d0d7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.groupBy(\"log_entry.properties.statusCode\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b15a9bf-4423-458d-9bdc-117a94465946",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.groupBy(\"log_entry.operation_name.value\", \"log_entry.properties.statusCode\").count().orderBy(\"value\", \"statusCode\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1f6a259-fdd9-4079-9fec-a69bba09300e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "With Azure, there is some time between when a `DELETE` operation request is received and when it completes.  This will help us see the distribution\n",
    "of minutes it takes to delete a VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4c3ddb9-06e1-4713-ac10-7cea0565e491",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "display(df.where(\"log_entry.operation_name.value == 'Microsoft.Compute/virtualMachines/delete'\").groupBy(\"vm_id\", \"log_entry.operation_name.value\").count().orderBy(F.col(\"count\").desc()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0da195ad-2938-40d7-8844-956f8702c1ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This is a quick check to see if there are any Databricks Instance Pools in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d521d51-de1c-41cb-9876-41eccb50777a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    df\n",
    "        .filter(\"log_entry.properties.responseBody.tags.DatabricksInstancePoolId IS NOT NULL\")\n",
    "        .groupBy(\"log_entry.properties.responseBody.tags.DatabricksInstancePoolId\")\n",
    "        .agg(F.count(\"vm_id\"), F.min(\"event_timestamp\"), F.max(\"event_timestamp\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "451d9f9e-b758-433e-85e9-4061753adbfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(activities.groupBy(\"tags.ClusterName\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57dfd8b4-b29c-49af-8b5a-69ef35456975",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we'll take a quick peek at the information we came here for... how many VM's of each type are in use at any given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69960436-806e-414b-865e-5f878a746af8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(observations.groupBy(\"vmSize\", \"date\").agg(F.sum(\"count\").alias(\"count\")).orderBy(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "283c9001-fb10-41fd-95d2-7d82e69b03f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(observations.groupBy(\"JobName\").agg(F.sum(\"count\").alias(\"vms\")).orderBy(F.col(\"vms\").desc()))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Acquire Data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
